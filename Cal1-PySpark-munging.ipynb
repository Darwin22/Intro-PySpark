{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false
   },
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> [pour Statistique et *Science des* grosses *Données*](https://github.com/wikistat/Intro-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trafic de données (*data munging*) avec <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé**: L'objectif de ce tutoriel est d'introduire les objets de la technologie [Spark](https://spark.apache.org/) et leur utilisation à l'aide de commandes en Python, plus précisément en utilisant l'API  [`PySpark`](http://spark.apache.org/docs/latest/api/python/). Motivations à l'utilisation de cet environnement qui distribue automatiquement les données sur un cluster et parallélise les tâches; description des principaux types de données et du concept de *Resilient Distributed Datasets* (RDD): toute tâche en *Spark* s'exprime comme la création, la transformation de RDDs puis le lancement d'actions sur ces RDDs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Pourquoi Spark\n",
    "\n",
    "Les données sont trop volumineuses pour la mémoire (RAM, disque) et/ou les temps de calcul rédhibitoires. Plus précisément, la distribution des données avec une technologies *Hadoop* est déjà en place ou devient incontournable. La description d'Hadoop n'est pas détaillée, non plus que les principes contraignants des fonctionnalités *MapReduce* associées. Consulter [Besse et al. 2016](https://hal.archives-ouvertes.fr/hal-01350099v2/document) pour une introduction à *MapReduce*, ses contraintes et une dicussion sur l'intérêt ou non de déployer Spark en fonction des traitements à mettre en oeuvre.\n",
    "\n",
    "*Rappelons le contexte:*\n",
    "- Les méthodes d'apprentissage statistique supervisée ou non déploient des algorithmes itératifs dont l'exécution sur des données gérées dans un cadre *Hadoop* provoquent des lectures / écritures à chaque itération. Les temps d'exécution s'en trouvent fortement pénalisés.\n",
    "- La technologie [Spark](http://spark.apache.org/) y remédie en intégrant le concept de *tables de données distribuées résilientes* (*Resilient Distributed Dataset* ou RDD de Zaharia et al. 2012). Très schématiquement, chaque partition de données reste en mémoire sur son serveur de calcul entre deux itérations tout en gérant les principes de tolérance aux pannes propres à Hadoop.\n",
    "- Les commandes spécifiques de *Spark*  s'exécutent en Java, Scala et aussi pour certaines en Python ou R. D'où l'intérêt de l'apprentissage de Python qui permet sans \"trop\" d'efforts de franchir le changement d'échelle en volume et vélocité, notamment par l'emploi de la librairie ou plutôt de l'interface de programmation (*Application Interface programmation* ou API) dédiée [`PySpark`](http://spark.apache.org/docs/latest/api/python/); cette API intère un noyau Jupyter pour une utilisation interactive sous forme de calepins.\n",
    "- [SparlSQL](https://spark.apache.org/sql/) permet d'accéder à tout type de données, structurées ou non, volumineuse (distribuées) en Python exécutant des commandes dans une syntaxe SQL.\n",
    "- [`MLlib`](http://spark.apache.org/docs/latest/ml-guide.html) intègre les principaux algorithmes d'apprentissage et méthodes statistique.\n",
    "- Deux autres librairies sont disponibles pour traiter des données en [flux](https://spark.apache.org/streaming/)  (*streaming*) ou celles de [graphes et réseaux](https://spark.apache.org/graphx/). Elles ne sont pas abordées dans ces tutoriels.\n",
    "\n",
    "Une évolution de *MLlib* est en cours à partir de la version 2. Elle permet d'intégrer progressivement les notions de *data frame* et *pipeline* aux algorithmes d'apprentissage machine déjà implémentées dans *MLlib* et applicables à des RDDs. Cette migration sera achevée pour la version 3.\n",
    "\n",
    " \n",
    "La principale motivation pour utiliser Spark est que les mêmes programmes ou commandes sont utilisées sur un poste isolé,  un cluster, un ensemble de machines virtuelles sur un serveur comme *Amazon Web Service*, *Google cloud*..., pour gérer prétraiter des données stockées dans un fichier ou distribuées dans un système *Hadoop*. \n",
    "\n",
    "**Attention**: (cf. [Besse et al. 2016](https://hal.archives-ouvertes.fr/hal-01350099v2/document)) Pour la phase d'apprentissage ou d'estimation de modèles, une solution qui consisterait à utiliser un plus gros ordinateur avec beaucoup de mémoire vive (architecture intégrée) est souvent  préférable en terme de temps de mise au point et d'exécution, à celle de déployer une architecture distribuée. L'utilisation de Spark / Hadoop s'impose lorsque les données sont déjà  dans cet environnement *mais*, très souvent, l'extraction,  l'échantillonnage, la préparation des données, en vue d'un problème spécifique suffit à se ramener à un volume compatible avec la mémoire d'un ordinateur. De plus dans le cas de données très volumineuse, la tentation est grande, ou il est indispensable, de considérer des modèles en très grande dimension avec énormément de paramètres. Dans ce cas, voir à ce propos les [ateliers consacrés aux données massives](https://github.com/wikistat/Ateliers-Big-Data), le stockage nécessaire du modèle dans chaque noeud ou ordinateur du cluster ou cloud fait planter Spark par défaut de mémoire; une architecture intégrée s'impose alors: *data munging* avec *Spark* mais [apprentissage](https://github.com/wikistat/Apprentissage) en Python avec `Scikit-learn`.\n",
    "\n",
    "Le principal **objectif** de ce tutoriel est d'introduire ces outils pour l'usage d'un statisticien ou maintenant *data scientist*. Ce premier tutoriel se focalise donc sur la préparation des données par la gestion de RDDs, classe d'objets encore très utilisée dans *Spark* pour le *data munging*. La classe *DataFrame* est définie et utilisée dans les tutoriels suivants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Présentation de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Documentation\n",
    "Les procédures d'installation, les concepts et les modes d'utilisation de [Spark](https://spark.apache.org/) sont explicités dans la [documentation](https://spark.apache.org/docs/latest/) en ligne et dans le livre de Karau et al. (2015)\\. \n",
    "\n",
    "### 2.2 Installation\n",
    "L'installation de Spark est plus ou moins complexe selon l'environnement système. Sous windows, elle nécessite, entre autres, l'instalaltion du gestionnaire de projet [Maven](https://maven.apache.org/) et d'une version récente de Java. SOus Ubuntu ou Mac Os, l'opéraiton est plus simple. Il suffit de télécharger, décompresser, installer dans un répertoire le source de *Spark* puis d'installer un fichier `kernel.json` dans le répertoire idoine afin de pouvoir utiliser `pyspark` dans Jupyter. Ces opérations relativement complexes, surtout sous Windows ou dans un *cloud*, ne sont pas détaillées car trop dépendantes de l'environnement. Des aides sont consultables sur internet.  Nous considérons que cela ne fait pas partie des compétences incontournables d'un statisticien qui se focalise sur les méthodes d'analyse / valorisation des données. Spark est donc supposé installé, accessible par l'intermédiaire de l'API *pyspark*. \n",
    "\n",
    "\n",
    "### 2.3 Utilisation\n",
    "Spark propose quatre types ou modes d'exécution:\n",
    "- *Standalone local* s'exécute dans un processus de machine virtuelle java sur un poste. C'est le mode utilisé pour tester un programme sur un petit ensemble de données et sur un poste de travail.\n",
    "- *Standalone cluster* est le cadre pour gérer en interne l'ordonnancement des tâches sur un cluster.\n",
    "- [*MESOS*](http://mesos.apache.org/) pour utiliser un cluster gérer par ces ressources du projet Apache.\n",
    "- [*YARN*](http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) même chose pour un cluster (*e.g* AWS) utilisant les fonctionnalités MapReduce de Hadoop.\n",
    "\n",
    "Spark exécute un programme en Java, Scala, Python ou R comme un langage interprété, à l'aide d'une console interactive, par le biais d'un notebook ou calepin Jupyter ou encore dans les environnements (nuages) loués comme par exemple à Amazon's EC2 (*Elastic Cloud Compute*). \n",
    "\n",
    "### 2.4 Configuration\n",
    "L'environnement utilisé est décrit par la commande `SparkContext` initialisant un objet `SparkConf` qui définit la configuration utilisée comme par exemple l'URL du noeud \"maître\" (*driver*) du cluster de calcul utilisé, le nombre de noeuds \"exécuteurs\" ou *workers*, leur espace mémoire réservé à chacun dans le cas de machines virtuelles. Chacun des exécuteurs doit avoir accès à l'installation de Python, ses librairies ainsi qu'à l'espace de sctockage où sont distribuées les RDDs.\n",
    "\n",
    "En utilisation sur un poste seul, cet environnement ne pose pas de problèmes de droits d'accès et est prédéfini à l'installation de Spark.\n",
    "\n",
    "C'est la version **Pyhton** 3 qui est utilisée.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Programme \"jouet\"\n",
    "Charger ce [calepin]() dans le répertoire de travail puis l'ouvrir dans l'environnement [*Jupyter*](http://jupyter.org/). Le fichier texte [`HistorCommande.csv`](https://www.math.univ-toulouse.fr/~besse/Wikistat/data/HistorCommande.csv) contient une liste de \"commandes\" d'épicerie:\n",
    "\n",
    "    Albert,cafe,3.27\n",
    "    Albert,chocolat,2.45\n",
    "    Julie,coca,3.20\n",
    "    Dominique,tarte,1.50\n",
    "    Paul,cassoulet,5.40\n",
    "\n",
    "Exécuter les cellules ci-dessous; les principaux objets seront détaillés ensuite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Chargement du fichier\n",
    "#Renseignez ici le dossier ou vous souhaitez stocker le fichier téléchargé.\n",
    "DATA_PATH=\"\" \n",
    "import urllib.request\n",
    "f = urllib.request.urlretrieve(\"http://www.math.univ-toulouse.fr/~besse/Wikistat/data/HistorCommande.csv\",DATA_PATH+\"HistorCommande.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture et distribution du fichier en RDD\n",
    "data = sc.textFile(DATA_PATH+\"HistorCommande.csv\").map(lambda line: \n",
    "        line.split(\",\")).map(lambda record: (record[0], record[1], record[2]))\n",
    "# Pas d'exécution tant qu'une action n'est pas requise\n",
    "# nombre total de commandes\n",
    "NbCommande=data.count()\n",
    "print(\"Nb de commandes: %d\" % NbCommande)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualiser le contenu du RDD\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de clients uniques\n",
    "ClientUnique = data.map(lambda record: record[0]).distinct().count()\n",
    "print(\"Nb clients: %d\" % ClientUnique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total des commandes\n",
    "TotalCom = data.map(lambda record: float(record[2])).sum()\n",
    "print(\"Total des commandes: %2.2f\" % TotalCom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produit le plus commandé\n",
    "produits = data.map(lambda record: (record[1], 1.0)).reduceByKey(lambda a, b: a + b).collect()\n",
    "plusFreq = sorted(produits, key=lambda x: x[1], reverse=True)[0]\n",
    "print(\"Produit le plus populaire: %s avec %d commandes\" %(plusFreq[0],plusFreq[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gestion élémentaire de RDDs\n",
    "La suite du tutoriel s'inspire de ceux proposés par [J. A. Dianes](https://github.com/jadianes/spark-py-notebooks) et utilise les données d'un concours: [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) concernant près de 9M d'interactions dans un réseau. Elles sont décrites en détail [ici](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names). L'objectif est d'apprendre à détecter des intrusions dans un réseau à partir d'un ensemble de variables ou *features* déjà calculées sur chaque transaction ou interaction avec le réseau.\n",
    "\n",
    "Un sous-échantillon est chargé localement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\",DATA_PATH+\"kddcup.data_10_percent.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Créer un RDD à partir du fichier\n",
    "Spark gère directement un fichier compressé.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = DATA_PATH+\"kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention** au principe d'*exécution paresseuse (lasy)* sur les RDDs. Aucune exécution tant qu'une action n'est pas lancée pour, par exemple, vérifier que le bon nombre de lignes sont lues. Autrement dit les commandes sont déclaratives jusqu'à une demande effective de traitement comme un affichage ou un comptage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ou en vérifiant le contenue des premières lignes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autre exemple : créer un RDD à partir d'une liste avec la transformation `parallelize` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = range(100)\n",
    "data = sc.parallelize(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant d'exécuter les mêmes actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***N.B.*** Bien faire la distinction entre une *transformation*, déclarative, et une *action* qui s'exécute et provoque l'exécution des transformations précédemment définies. Une transformation est une instruction *paresseuse* (*lazy*) qui n'est exécutée que lorsque cela est nécessaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Filtrer un RDD\n",
    "Une fonction logique est évaluée afin d'extraire les bons éléments d'un RDD pour en créer un nouveau. Dans l'exemple ci-dessous, il s'agit de compter le nombre de transactions \"normales\". La commande suivante déclare la transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_raw_data = raw_data.filter(lambda x: 'normal.' in x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "qui est ensuite exécuté par l'action `count`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "normal_count = normal_raw_data.count()\n",
    "tt = time() - t0\n",
    "print(\"Il y a {}  interactions 'normales'\".format(normal_count))\n",
    "print(\"Calcul réalisé en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Transformer un RDD: transformation *map*\n",
    "Comme précédemmnet, la fonction `lambda` de python est utilisée pour être appliquée dans une transformation *map* à chaque élément de la table. Il s'agit d'extraire chaque champ avec le séparateur \",\" du format .csv. La première ligne du RDD créé est ensuite affichée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "t0 = time()\n",
    "head_rows = csv_data.take(5)\n",
    "tt = time() - t0\n",
    "print(\"Parse completed in {} seconds\".format(round(tt,3)))\n",
    "pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La commande suivante prend plus de temps car elle est exécutée à plus de lignes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "head_rows = csv_data.take(10000)\n",
    "tt = time() - t0\n",
    "print(\"Parse completed in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 *Map* d'une fonction prédéfine\n",
    "La transformation *map* applique une fonction prédéfinie à tout le RDD. L'objectif est de définir ci-dessous l'option transaction \"normale\" comme \"valeur de clef\" associée à une valeur qui est la liste de tous les autres éléments de la ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fontion qui sépare les champ (elems=valeur) et extrait la 41ème = clef.\n",
    "def parse_interaction(line):\n",
    "    elems = line.split(\",\")\n",
    "    tag = elems[41]\n",
    "    return (tag, elems)\n",
    "# Affichage des 5 premiers\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "head_rows = key_csv_data.take(5)\n",
    "pprint(head_rows[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est ensuite possible de dénombrer par valeur de clef."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 L'action `collect`\n",
    "En plus des actions `count`et `take`, celle `collect` a pour effet de charger en mémoire tous les éléments d'un RDD. A manier avec prudence pour éviter des dépassements de mémoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "all_raw_data = raw_data.collect()\n",
    "tt = time() - t0\n",
    "print(\"Data collected in {} seconds\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela prend plus de temps pour collecter tous les fragments détenus par les exécuteurs et les regrouper (*reduce*) ensemble. \n",
    "\n",
    "Le dernier exemple combine les fonctions précédentes pour collecter toutes les interactions normales comme des paires (clef, valeur)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data from file\n",
    "data_file = DATA_PATH+\"kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)\n",
    "\n",
    "# parse into key-value pairs\n",
    "key_csv_data = raw_data.map(parse_interaction)\n",
    "\n",
    "# filter normal key interactions\n",
    "normal_key_interactions = key_csv_data.filter(lambda x: x[0] == \"normal.\")\n",
    "\n",
    "# collect all\n",
    "t0 = time()\n",
    "all_normal = normal_key_interactions.collect()\n",
    "tt = time() - t0\n",
    "normal_count = len(all_normal)\n",
    "print(\"Data collected in {} seconds\".format(round(tt,3)))\n",
    "print(\"There are {} 'normal' interactions\".format(normal_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette procédure est plus longue que la simple application de l'action `count`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Opération sur des RDDs\n",
    "### 4.1 Opérations ensemblistes\n",
    "Des opérateurs ensemblistes: `subtract`, `distinct`, et `cartesian`, sont des transformations qui s'appliquent à des RDDs de même type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relecture des données\n",
    "data_file = DATA_PATH+\"kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.1 `Substract`\n",
    "Obtenir les intrusions ou attaques en retranchant l'ensemble des ineractions \"normales\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Interactions normales comme précédemment\n",
    "normal_raw_data = raw_data.filter(lambda x: \"normal.\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ensemble des intrusions\n",
    "attack_raw_data = raw_data.subtract(normal_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "# count all\n",
    "t0 = time()\n",
    "raw_data_count = raw_data.count()\n",
    "tt = time() - t0\n",
    "print(\"All count in {} secs\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count normal\n",
    "t0 = time()\n",
    "normal_raw_data_count = normal_raw_data.count()\n",
    "tt = time() - t0\n",
    "print(\"Normal count in {} secs\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count attacks\n",
    "t0 = time()\n",
    "attack_raw_data_count = attack_raw_data.count()\n",
    "tt = time() - t0\n",
    "print(\"Attack count in {} secs\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Il y a {} interactions normales et {} attaques, \\\n",
    "pour un total de {} interactions\".format(normal_raw_data_count,attack_raw_data_count,raw_data_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le résultat fournit deux RDDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2 `cartesian`\n",
    "Le produit cartésien retourne toutes les paires possibles entre éléments de deux RDDS. C'est utilisé pour générer toutes les paires entre \"service\" (2ème colonne) et \"protocole\" (3ème colone). \n",
    "\n",
    "La transformation `distinct` permet de lister les valeurs possibles (uniques) de chaque variable qualitative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour le protocole\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "protocols = csv_data.map(lambda x: x[1]).distinct()\n",
    "protocols.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour le service\n",
    "services = csv_data.map(lambda x: x[2]).distinct()\n",
    "services.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le produit cartésien fournit tous les couples possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product = protocols.cartesian(services).collect()\n",
    "print(\"Il y a {} combinaisons de protocol X service\".format(len(product)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A utiliser avec prudence si les RDDs sont volumineuses avec beaucoup de modalités. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Aggrégations de RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'autres manipulations ou agrégations sont produites par des actions `reduce`, `fold`, et la dernière: `aggregate` qui est une forme de généralisation.\n",
    "\n",
    "#### 4.2.1 `reduce`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Relecture des données\n",
    "data_file = DATA_PATH+\"kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les actions `fold` et `reduce`prennent une fonction comme argument (par exemple \"+\") qui est appliquée à deux éléments d'un RDD; `fold` diffère de `reduce` car nécessite une initialisation préliminaire (par exemple 0  pour \"+\").\n",
    "\n",
    "L'exemple calcule les sommes des temps d'interactions par statut: normales ou non."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse data\n",
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "\n",
    "# Séparation en deux RDDs\n",
    "normal_csv_data = csv_data.filter(lambda x: x[41]==\"normal.\")\n",
    "attack_csv_data = csv_data.filter(lambda x: x[41]!=\"normal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions transférées à `reduce` envoie et retourne des RDDs de même type sous la forme d'un nouveau RDD à .créer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Création de ces RDDs avec les durées\n",
    "normal_duration_data = normal_csv_data.map(lambda x: int(x[0]))\n",
    "attack_duration_data = attack_csv_data.map(lambda x: int(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La transformation `reduce` appliquée aux deux nouveaux RDDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_normal_duration = normal_duration_data.reduce(lambda x, y: x + y)\n",
    "total_attack_duration = attack_duration_data.reduce(lambda x, y: x + y)\n",
    "\n",
    "print(\"Total duration for 'normal' interactions is {}\".\\\n",
    "    format(total_normal_duration))\n",
    "print(\"Total duration for 'attack' interactions is {}\".\\\n",
    "    format(total_attack_duration))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associée à `count` pour calculer la moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_count = normal_duration_data.count()\n",
    "attack_count = attack_duration_data.count()\n",
    "\n",
    "print(\"Mean duration for 'normal' interactions is {}\".\\\n",
    "    format(round(total_normal_duration/float(normal_count),3)))\n",
    "print(\"Mean duration for 'attack' interactions is {}\".\\\n",
    "    format(round(total_attack_duration/float(attack_count),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donne une première discrimination simpliste entre interactions.\n",
    "\n",
    "#### 4.2.2 `aggregate`\n",
    "\n",
    "L'action `aggregate` relaxe la contrainte de gérer des RDDs de même type mais nécessite l'initialisation comme `fold`. Deux fonctions sont définies, la première du RDD avec l'accumulateur. La deuxième fusionne deux accumulateurs. L'objectif reste de calculer les moyennes des durées par statut.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_sum_count = normal_duration_data.aggregate((0,0), # valeurs initiales à 0\n",
    "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # Somme des durées et cumul des interactions\n",
    "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # cumul des accumualteurs\n",
    "    )\n",
    "print(\"Durée moyenne des interactions normales {}\".\\\n",
    "    format(round(normal_sum_count[0]/float(normal_sum_count[1]),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le premier accumulateur somme les durées tandis que le 2ème compte le nombre d'interactions. Combiner un accumlateur avec un RDD consiste à sommet la valeur et incrémenter le comptage. Combiner deux accumulateurs est une simple somme par paire.\n",
    "\n",
    "Même chose avec les attaques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_sum_count = attack_duration_data.aggregate(\n",
    "    (0,0), # the initial value\n",
    "    (lambda acc, value: (acc[0] + value, acc[1] + 1)), # combine value with acc\n",
    "    (lambda acc1, acc2: (acc1[0] + acc2[0], acc1[1] + acc2[1])) # combine accumulators\n",
    "    )\n",
    "\n",
    "print(\"Durée moyenne des interactions agressives {}\".\\\n",
    "    format(round(attack_sum_count[0]/float(attack_sum_count[1]),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Combiner par clef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark fournit des fonctions pour gérer des paires (clef, valeur), bases du *mapreduce*. Elles exécutent des agrégations et autres traitements par clef.\n",
    "\n",
    "Ces fonctions permettent d'explorer efficacement différents croisement avec le type d'interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# relecture des données\n",
    "data_file = DATA_PATH+\"kddcup.data_10_percent.gz\"\n",
    "raw_data = sc.textFile(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit donc, pour explorer, de croiser le type d'interaction avec d'autres variables comme la durée. Ceci est initié par la création de RDDs où la clef est le type d'interaction tandis que la liste des autres champs est la valeur. \n",
    "\n",
    "La création des RDDs de paires (clef, valeur) est obtenue par l'applicaiton d'une transformation *map*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_data = raw_data.map(lambda x: x.split(\",\"))\n",
    "key_value_data = csv_data.map(lambda x: (x[41], x)) # x[41] contient le type normal ou non"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage du premier élément."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_data.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Aggrégation de RDDs par clef/valeur\n",
    "\n",
    "Spark fournit des fonctions spécifiques adaptées aux paires (clef, valeurs). \n",
    "\n",
    "Ainsi la transformation `reduceByKey` est utilisée pour calculer la durée par type d'interaction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_value_duration = csv_data.map(lambda x: (x[41], float(x[0]))) \n",
    "durations_by_key = key_value_duration.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "durations_by_key.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le comptage est aussi opéré par clef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_key = key_value_data.countByKey()\n",
    "counts_by_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 `combineByKey`\n",
    "C'est la fonction d'aggrégation par clef la plus générale et est utilisée par la plupart des autres fonctions de combinaison par clef. Comme pour la transformation `aggregate` les valeurs en retour ne sont pas nécessairement du même type que les données en entrée. \n",
    "\n",
    "Toujours pour calculer la moyenne des durées par type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_counts = key_value_duration.combineByKey(\n",
    "    (lambda x: (x, 1)), # valeur initiale x and compteur 1\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # Combiner une paire  avec une paires d'accumulateurs (somme et incrément)\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combinaison des accumulateurs\n",
    "     )\n",
    "sum_counts.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les divisions produisent les moyennes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_means_by_type = sum_counts.map(lambda lambda_args: (lambda_args[0], round(lambda_args[1][0]/lambda_args[1][1],3))).collectAsMap()\n",
    "\n",
    "#Print them sorted\n",
    "for tag in sorted(duration_means_by_type, key=duration_means_by_type.get, reverse=True):\n",
    "    print(tag, duration_means_by_type[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PySpark (Spark 2.3.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "nav_menu": {
    "height": "492px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
