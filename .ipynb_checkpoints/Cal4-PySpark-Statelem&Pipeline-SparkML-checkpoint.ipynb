{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des grosses data](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à la librairie  [SparkML](https://spark.apache.org/docs/latest/ml-guide.html) (ou MLlib  DataFrame-based API) de  <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depuis Spark 2.0 la librairie MLlib, qui maniupule éxclusivement des RDD est maintenant en maintenance. Elle peut toujours être utilisé, mais il n'y aura pas de nouveaux outils développé pour cette librairie.\n",
    "\n",
    "La principale librairie de machine learning pour spark est maintenant SparkML. SparkML est une librairie qui manipule exclusivement des DataFrame, plus facile d'utilisation que les RDD et qui permet l'utilistation d'autres services spark tel que Spark Datasources, SQL queries etc...\n",
    "Si MLlib est aujourd'hui plus complète, SparkML devrait posséder exactement les même fonctions que MLlib dans la version 2.3 de Spark (Aujourd'hui la version 2.2 est disponible). La librairie MLlib, qui manipule exclusivement des RDD n'existera plus dans Spark 3.0\n",
    "\n",
    "\n",
    "SparkML n'est pas le nom définitif que portera la librairie de machine learning de spark. C'est un nom temporaire créer pour déveloper un outil basé sur les DataFrame. MLlib sera le nom définitif de cette librairie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé**: Ce tutoriel continue l'initiation à [Spark](https://spark.apache.org/) à l'aide de commandes en Python en utilisant l'API  [`PySpark`](http://spark.apache.org/docs/latest/api/python/). Dans ce calepein Nous allons manier majoritairement des DataFrame telles que celles décrite dans le calepin précédent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Statistique élémentaire avec <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> et  [SparkML](https://spark.apache.org/docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La plupart des fonctions de statistiques élémentaires de <I>MLlib</I>, décrites dans le calepin 2 de cette introduction à pyspark ne sont pas encore disponible dans la librairie <I>SparkML</I>. Seul la fonction de correlation et le test d'hypothèse du χ² sont disponible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "La fonction <I> pyspark.ml.stat.Correlation</I> permet de calculer la correlation entre chaque colonne d'une DataFrame. Les correlations disponible sont celles de <I>Pearson</I> et <I>Spearman</I>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Test d'Hypothèse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark.ml supporte actuellement le test du Chi-2 de Pearson. Ce test permet d'effectuer un test d'indépendance pour chaque <I>features</I> vis à vis du <I>label</I>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "\n",
    "data = [(0.0, Vectors.dense(0.5, 10.0)),\n",
    "        (0.0, Vectors.dense(1.5, 20.0)),\n",
    "        (1.0, Vectors.dense(1.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 30.0)),\n",
    "        (0.0, Vectors.dense(3.5, 40.0)),\n",
    "        (1.0, Vectors.dense(3.5, 40.0))]\n",
    "df = spark.createDataFrame(data, [\"label\", \"features\"])\n",
    "\n",
    "r = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "print(\"pValues: \" + str(r.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r.statistics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librairie <I>SparkML</I>, contrairement à <I>MLlib</I>, est basé sur la notion de <B>ML Pipeline</B>. \n",
    "\n",
    "Un <B>ML Pipeline</B> permet de combiner différentes étapes de traitement, allant du nettoyage des données jusqu'a l'étape d'apprentissage en un seul objet appelé <I>pipeline<\\I> ou <I>workflow.<\\I>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator, Transformer, and Param\n",
    "\n",
    "La construction d'un <B>ML Pipeline</B> est effectué à partir de trois type d'objets décrits ci-dessous :\n",
    "\n",
    "\n",
    " * <B>Transformer</B>: Un <B>Transformer</B> est un algorithme qui permet de transformer une DataFrame en une autre DataFrame. Dans la pluspart des cas, la nouvelle DataFrame est une DataFrame identique à la première avec une colonne supplémentaire. Exemple de <B>Transformer</B>: \n",
    "     * Un modèle d'apprentissage va prendre en entrée une DataFrame de variable et retourner une nouvelle DataFrame avec les variables et une nouvelle colonne correspondant à la prédiction.\n",
    "     * Le Transformer <I>StringIndexer</I> va prendre en entrée une DataFrame possédant un colonne de texte et retourner une nouvelle DataFrame avec la même colonne texte et une nouvelle colonne ou les textes seront remplacés par une valeur numérique.\n",
    " \n",
    "\n",
    "* <B>Estimator</B>: Un <B>Estimator</B> est un algorithme qui peut-être appliquer sur une DataFrame afin de produire un <B>Transformer</B>. Exemple d'<B>Estimator</B>:\n",
    "    * Un algorithme d'apprentissage est un <B>Estimator</B>. Une fois celui-ci appliqué sur une DataFrame, il va produire un modèle d'apprentissage qui sera un <B>Transformer</B>, comme décrit précédemment.\n",
    "\n",
    "\n",
    " * <B>Parameter</B>: Chaque  <B>Transformer</B> et <B>Estimators</B> partage une même API pour spécifier leur paramètre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple : Regression Logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# DataFrame D'apprentissage\n",
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# On crée d'abord un objet LogisticRegression en spécifiant ses paramètres. Cet objet est un Estimator\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01, featuresCol='features', labelCol='label', predictionCol='prediction', probabilityCol='probability')\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")\n",
    "\n",
    "# On applique l'Estimator sur notre DataFrame d'apprentissage. L'objet qui en résulte, le modèle d'apprentissage, est un Transformer.\n",
    "model = lr.fit(training)\n",
    "\n",
    "\n",
    "# DataFrame de Test\n",
    "test = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n",
    "\n",
    "# On applique le Transformer sur la DataFrame de Test 'test'.\n",
    "prediction = model.transform(test)\n",
    "\n",
    "# Il en resulte une nouvelle DataFrame 'prediction' qui correspond a la DataFrame 'tes' auquelle on été ajoutés une colonne \"prediction\" et une colonne \"probability\".\n",
    "# Les noms de ces nouvelles colonnes ont été spécifiés dans les paramètres lors de l'instanciation de l'Estimator 'lr' puis transmis au Transformer 'model'.\n",
    "\n",
    "# Nous pouvons maintenant illustrer les résultats.\n",
    "result = prediction.select(\"features\", \"label\", \"probability\", \"prediction\") \\\n",
    "    .collect()\n",
    "\n",
    "for row in result:\n",
    "    print(\"features=%s, label=%s -> prob=%s, prediction=%s\"\n",
    "          % (row.features, row.label, row.probability, row.prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un <B> Pipeline </B> est un enchainement de plusieurs <B> Transformers </B> et <B> Estimators </B> afin de spécifier un processus entier de Machine Learning. Par exemple, pour effectuer de l'apprentissage statistique sur des données textuelles, ces différentes étapes sont appliquées les unes à la suite des autres: \n",
    "\n",
    " * Découpage du texte en liste de mot\n",
    " * Conversion en variable numérique\n",
    " * Aprentissahe sur les données numérique\n",
    " * Prédiction \n",
    "\n",
    "Toutes ces étapes peuvent être résumé dans un seul objet appelé <B> Pipeline </B>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemple : Toeknize, Hash et Regression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "\n",
    "# DataFrame d'Apprentissage\n",
    "training = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "# Configure un pipeline qui consiste en 3 étapes :\n",
    "\n",
    "# 1/ Tokenizer. On spécifie la colonne d'entrée 'text' et la colonne de sortie 'words'.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "# 2/ Hash. La colonne d'entrée est spécifié ici comme étant la colonne de sortie de l'étape précédente.\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "# 3/ Regression Logistique\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "\n",
    "# On configure le Pipeline comme la succession des étapes précédentes.\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# On applique toutes les étapes sur la DataFrame d'apprentissage.\n",
    "model = pipeline.fit(training)\n",
    "\n",
    "# DataFrame Test.\n",
    "test = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Prediction.\n",
    "prediction = model.transform(test)\n",
    "selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n",
    "for row in selected.collect():\n",
    "    rid, text, prob, prediction = row\n",
    "    print(\"(%d, %s) --> prob=%s, prediction=%f\" % (rid, text, str(prob), prediction))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PySpark (Spark 2.2.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "nav_menu": {
    "height": "457px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
