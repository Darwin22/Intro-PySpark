{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des grosses data](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La classe Data Frame de <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> [SQL](http://spark.apache.org/sql/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé**: Ce tutoriel introduit la classe *data frame* proposée par la librairie [*SparkSQL*](http://spark.apache.org/sql/). Cette classe deviendra un standard pour toutes les manipulations de données structurées à partir de la version 3.0 de *Spark*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture des données\n",
    "\n",
    "Ce tutoriel s'inspire de ceux proposés par [J. A. Dianes](https://github.com/jadianes/spark-py-notebooks) pour l'utilisation des données du concours [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) concernant près de 9M d'interactions dans un réseau. Elles sont décrites en détail [ici](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names). L'objectif est d'apprendre à détecter des intrusions dans un réseau à partir d'un ensemble de variables ou *features* déjà calculées sur chaque transaction ou ineraction avec le réseau.\n",
    "\n",
    "Un sous-échantillon est chargé localement avant de créer la RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=\"\" \n",
    "\n",
    "import urllib.request\n",
    "# Download data\n",
    "urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\",DATA_PATH+\"kddcup.data_10_percent.gz\")\n",
    "urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\",DATA_PATH+\"kddcup.names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lis et créé la liste des colonne du fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read col names\n",
    "file_names = open(DATA_PATH+\"kddcup.names\",\"r\").readlines()\n",
    "col_names = [k.split(\":\")[0] for k in file_names[1:]]+[\"interactions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL\n",
    "\n",
    "*Spark SQL* est un module de spark permettant de manipuler des données structurées, contrairement au RDD. En effet Spark ne fait pas de différences entre un RDD consituté de listes ou de dictionnaires par exemple. Dans *Spark SQL* les données sont structurées selon un *schema*. Grace à cette structure plus d'optimisation peuvent être opéré sur les calculs.\n",
    "Il existe plusieur façon d'utiliser ces données structurés : en utilisant directement des requêtes SQL, l'API dataset (disponible via l'API java et scala uniquement) et les Dataframe disponible avec pyspark. C'est cette dernière que nous allons voir plus en détail dans ce calepin.\n",
    "\n",
    "\n",
    "Le point d'entrée dans les fonctionnalités SQL en *Spark* est la classe *SQLContext*. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Construire un Data Frame\n",
    "\n",
    "Un *Data Frame* Spark est une collection de données distribuées et organisées en *colonnes* identifiées par des noms. C'est conceptuellement  équivalent à une table dans une base de données relationnelle, un *data frame* en R ou en Python-pandas. Cette classe peut être obtenue de sources ou types de données variés : *Hive*, *json*, *xml*, *parquet*, *cassandra*... Ces fonctionnalités ne sont pas toutes introduites mais elles constituent un atout évident pour justifier du développement de cet environnement. Nous allons voir dans cette partie comment créer une DataFrame à partir d'un RDD, d'un fichier *csv* et d'une *DataFrame* pandas. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depuis une RDD\n",
    "\n",
    "Lecture du fichier dans une RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = DATA_PATH+\"kddcup.data_10_percent.gz\"\n",
    "# Creation d'une RDD de string.\n",
    "string_rdd = sc.textFile(data_file).cache()\n",
    "# Creation d'une RDD de Liste de string.\n",
    "list_rdd = string_rdd.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Construction des Row (lignes, une à une)\n",
    "La première opération consiste à construire le schéma des données. \n",
    "\n",
    "SparkSQL convertit en *data frame* un RDD composés d'objets *Row*.\n",
    "Un *Row* est comparable à un dictionnaire. Cet objet est construit en passant une liste de (clef, valeur) comme [kwargs](http://deusyss.developpez.com/tutoriels/Python/args_kwargs/).  La clef définit le nom de colonne et le type (entier, flottant...) est déduit de la première ligne. Il est donc important qu'il n'y ait pas de données manquantes dans la première ligne du RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "row_rdd = list_rdd.map(lambda p: Row(\n",
    "    duration=int(p[0]), \n",
    "    protocol_type=p[1],\n",
    "    service=p[2],\n",
    "    flag=p[3],\n",
    "    src_bytes=int(p[4]),\n",
    "    dst_bytes=int(p[5]),\n",
    "    interactions = p[-1],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois le RDD créé par lignes, le schéma est inféré puis enregistré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd_1 = sqlContext.createDataFrame(row_rdd)\n",
    "df_rdd_1.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spécifier le shéma\n",
    "\n",
    "Il est également possible de spécifier le shéma présent dans une *RDD* avant de la convertir en *Dataframe*\n",
    "Le schéma est crée grâce à l'objet *StructType* composé de *StructField* qui décrit les champs de valeur rencontré dans la *rdd*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "fields = [StructField(\"duration\", IntegerType(), True),\n",
    "         StructField(\"protocol_type\", StringType(), True),\n",
    "          StructField(\"service\", StringType(), True),\n",
    "          StructField(\"flag\", StringType(), True),\n",
    "          StructField(\"src_bytes\", IntegerType(), True),\n",
    "          StructField(\"dst_bytes\", IntegerType(), True),\n",
    "          StructField(\"interactions\", StringType(), True)]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subList_rdd = list_rdd.map(lambda p: (int(p[0]), p[1], p[2], p[3], int(p[4]), int(p[5]), p[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd = sqlContext.createDataFrame(subList_rdd, schema)\n",
    "df_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depuis un csv\n",
    "\n",
    "Les méthodes précédentes impliquent de connaitre la structure présente dans les RDD afin de spécifier le *schema*.\n",
    "\n",
    "Lorsque le fichier source est dans un certain format connu (*parquet*, *json*, *csv*), on peut utiliser la fonction *spark.read.load* afin d'inférer automatiquement ce format dans une dataframe.\n",
    "\n",
    "Le fichier *kddcup.data_10_percent.gz* est organisé comme un fichier *csv*. La Dataframe peut donc directement être lu à partir du fichier. Cela permet également d'intégrer toutes les colonnes du fichiers dans une dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_csv = spark.read.load(data_file, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"False\")\n",
    "#Specify columns names\n",
    "df_csv=df_csv.toDF(*col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depuis une DaframePandas\n",
    "\n",
    "\n",
    "La conversion d'un *DataFrame pandas* vers un *Dataframe PysSpark* peut être effectuer grâce à la librairie *pyarrow* qui permet des transferts d'objets entre la *JVM* et *python*.\n",
    "\n",
    "Pour l'utiliser il est necessaire de changer la configuration `spark.sql.execution.arrow.enabled` à `true`. Celle-ci est à `False`par défaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture du fichier dans un *DataFrame pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.read_csv(DATA_PATH+\"kddcup.data_10_percent.gz\", sep=\",\", names=col_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(pandas_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple de requête SQL\n",
    "\n",
    "\n",
    "*SparkSQL* permet d'appliquer une reqête SQL sur un objet et de retourner le resultats ce cette requête sous le format *DataFrame*\n",
    "\n",
    "Pour cela, on enregistre dans un premier temps la *DataFrame* sous le format *SQL temporary view*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.createOrReplaceTempView(\"interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des requêtes SQL peuvent ensuite être exécutées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les interactions \"tcp\" de plus de 1 s et sans transfert.\n",
    "tcp_interactions = sqlContext.sql(\"\"\"\n",
    "    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND duration > 1000 AND dst_bytes \n",
    "    = 0\"\"\")\n",
    "tcp_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats d'une requête SQL sont des *DataFrame*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tcp_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sortie des durées avec les dst_bytes\n",
    "tcp_interactions_out = tcp_interactions.rdd.map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))\n",
    "for ti_out in tcp_interactions_out.collect():\n",
    "  print (ti_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impression du schéma du *data frame*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp_interactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Requêtes en tant qu'opérations sur DataFrame\n",
    "\n",
    "SparkSQL inclut un langage pour la manipulation de données structurées. Il permet de combiner des méthodes de sélection, filtrage, regroupement... des données. \n",
    "\n",
    "### DataFrame operation\n",
    "\n",
    "*printSchema* permet d'afficher le type de chaque colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*select* permet d'extraire les colonnes d'une dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.select(\"interactions\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.select(\"interactions\",\"duration\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*groupBy* est équivalent à la fonction *groupby* de pandas.\n",
    "L'exemple ci-dessous compte le nombre d'interactions par type de protocole à l'aide de la fonction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "df_rdd.groupBy(\"protocol_type\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Requete executee en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*filter* permet de selectionner les lignes d'une *DataFrame* selon une condition fixé sur une colonne\n",
    "\n",
    "Pour compter les interactions de moins d'une seconde sans transfert de données et groupés par protocole, il suffit d'ajouter des filtres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "df_rdd.filter(df_rdd.duration>1000).filter(df_rdd.dst_bytes==0).groupBy(\"protocol_type\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Requete executee en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *map* et *User defined function*\n",
    "\n",
    "La fonction *map* n'est pas disponible sur les objets *DataFrame*. Une première façon consiste à convertir la *DataFram* en objet rdd, puis d'appliquer la fonction *map*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.rdd.map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes)).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre façon consiste à convertir la fonction *lambda* en *user defined function*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "function = udf(lambda x,y: \"Duration: {}, Dest. bytes: {}\".format(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce format permet ensuite d'appliquer cette fonction à la colonne de la *Dataframe* désiré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataframe = df_rdd.select(function(\"duration\",\"dst_bytes\").alias(\"string_output\"))\n",
    "output_dataframe.take(3)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation d'une colonne\n",
    "\n",
    "Dans la section précedente nous avons vu comment appliquer une fonction *udf* sur une *Dataframe*. Le résultat de cette fonction est une *Dataframe* d'une colonne. \n",
    "Il est possible d'ajouter cette colonne à la dataframe existante grâce à la fonction *withColumn*\n",
    "\n",
    "\n",
    "Pour illustrer cette fonction, nous allons ajouter une colonne `label` à la *dataframe* `df`\n",
    " qui label les interactions en deux catégories: *attack* et *normal*. \n",
    "\n",
    "On définit dans un premier temps la fonction permettant de catégoriser chaque interaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attack_or_normal_func(s):\n",
    "    return \"normal\" if s == \"normal.\" else \"attack\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on convertit cette fonction en *user defined function*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attack_or_normal = udf(attack_or_normal_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On créer une nouvelle colonne `label` a partir du resultat de cette fonction à l'aide de la fonction *withColumn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_label = df.withColumn(\"label\", attack_or_normal(df.interactions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vérifie que la colonne \"label\" a bien été créé. Si on avait choisi comme nom de colonne, une colonne déjà existante, celle-ci aurait été remplacée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_label.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut alors dénombrer le nombre d'attaque et d'interation normales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "df_with_label.select(\"label\").groupBy(\"label\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Requete executee en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dénombrement par label et type de protocole pour souligner le pouvoir discriminant de cette variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "df_with_label.select(\"label\", \"protocol_type\").groupBy(\"label\", \"protocol_type\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Requete executee en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que dire du protocole `udp`?\n",
    "\n",
    "Ajouter la prise en compte du transfert de données à partir de la cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "df_with_label.select(\"label\", \"protocol_type\", \"dst_bytes\").groupBy(\"label\", \"protocol_type\", df_with_label.dst_bytes==0).count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Requete executee en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Consulter les très nombreuses autres fonctionnalités, présentes ou à venir (version 2.3) dans la [documentation en ligne](http://spark.apache.org/docs/latest/sql-programming-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas UDF\n",
    "\n",
    "Une *pandas udf* est simialire à une *udf* définie précédemment. Mais elle permet de prendre en entrée une colone d'un *dataframe* qui sera traité comme un *Series* de *pandas*.\n",
    "\n",
    "Cette nouvelle classe de fonction permet une amélioration significative des [performances](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html).\n",
    "\n",
    "Il existe deux types de *pandas udf* : `Scalar` et `Grouped Map`.\n",
    "\n",
    "#### Scalar\n",
    "\n",
    "Les *Scalar Pandas UDFs* sont utilisé pour vectoriser efficactement des opération scalaire. Elle prend en compte une *Series* de *pandas* comme arguement et retourne une autre *Series* de *pandas* de la même taille tout en benificiant des fonction native de pandas. \n",
    "\n",
    "Nous illustrons cette exemple avec la fonction `cumsum`de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def cum_sum(x):\n",
    "    return x.cumsum()\n",
    "\n",
    "cum_sum_udf = pandas_udf(cum_sum, returnType=IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_sum_duration = df.select(cum_sum_udf(col(\"duration\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_sum_duration.take(1000)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouped Map\n",
    "\n",
    "Les fonctions *Grouped map Pandas UDFs* sont à utiliser avec les fonctions `groupBy().apply()` qui permet d'appliquer le pattern “split-apply-combine” qui consiste en trois étapes :\n",
    "\n",
    " * *Split* les données en groupe à l'aide de `DataFrame.groupBy`.\n",
    " * *Apply* la fonction sur chaque groupe. Les entrées et les sorties de la fonction sont des DataFrame de Pandas. Les entrées de cette étape contiennent toutes les lignes et colonnes de chaque groupes. \n",
    " * *Combine* les résultats dans une nouvelle `DataFrame`.\n",
    "\n",
    "Afin d'utiliser `groupBy().apply()`, nous devons définir:\n",
    "\n",
    "* Une fonction python qui définit le calcul à effectuer sur chaque groupe.\n",
    "* Un objet *StructType*  ou un *string* qui définit le schema de la *DataFrame* de sortie.\n",
    "\n",
    "L'exemple suivant montre comment soustraire à chaque groupes (\"attack\" ou \"normal\") la valeur moyenne de la durée pour chacun de ces groupes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"label string, duration int\", PandasUDFType.GROUPED_MAP)\n",
    "def substract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    duration = pdf.duration\n",
    "    return pdf.assign(duration=duration - duration.mean())\n",
    "\n",
    "df_with_label.select(\"label\",\"duration\").groupby(\"label\").apply(substract_mean).show()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PySpark (Spark 2.3.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
