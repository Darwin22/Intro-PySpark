{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> [pour Statistique et *Science des* grosses *Données*](https://github.com/wikistat/Intro-Python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La classe *DataFrame* de <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"Spark\"/> </a> [SQL](http://spark.apache.org/sql/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Résumé**: Ce tutoriel introduit la classe *DataFrame* proposée par la librairie [*SparkSQL*](http://spark.apache.org/sql/). Cette classe deviendra un standard pour toutes les manipulations de données structurées à partir de la version 3.0 de *Spark*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "### 1.1 Lecture des données\n",
    "\n",
    "Ce tutoriel s'inspire de ceux proposés par [J. A. Dianes](https://github.com/jadianes/spark-py-notebooks) pour l'utilisation des données du concours [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) concernant près de 9M d'interactions dans un réseau. Elles sont décrites en détail [ici](http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names). L'objectif est d'apprendre à détecter des intrusions dans un réseau à partir d'un ensemble de variables ou *features* déjà calculées sur chaque transaction ou ineraction avec le réseau.\n",
    "\n",
    "Un sous-échantillon est chargé localement avant de créer la RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=\"\" \n",
    "\n",
    "import urllib.request\n",
    "# Download data\n",
    "urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\",DATA_PATH+\"kddcup.data_10_percent.gz\")\n",
    "urllib.request.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.names\",DATA_PATH+\"kddcup.names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lis et créé la liste des colonne du fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read col names\n",
    "file_names = open(DATA_PATH+\"kddcup.names\",\"r\").readlines()\n",
    "col_names = [k.split(\":\")[0] for k in file_names[1:]]+[\"interactions\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 2 *Spark SQL*\n",
    "\n",
    "*Spark SQL* est un module de Spark permettant de manipuler des données structurées, contrairement au RDD. En effet Spark ne fait pas de différences entre un RDD consituté de listes ou de dictionnaires par exemple. Dans *Spark SQL* les données sont structurées selon un *schéma*. Grâce à cette structure plus d'optimisations peuvent être opérées sur les calculs.\n",
    "Il existe plusieurs façons de manipuler ces données structurées, notamment directement avec des requêtes SQL ou avec l'API `dataset` (disponible via les APIs java et scala uniquement) ou encore avec les *DataFrames* disponibles par `pyspark`. C'est cette dernière option qui est détaillée dans ce calepin.\n",
    "\n",
    "Le point d'entrée dans les fonctionnalités SQL en *Spark* est la classe *SQLContext*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 Construire un *DataFrame*\n",
    "\n",
    "Un *DataFrame* Spark est une collection de données distribuées et organisées en *colonnes* identifiées par des noms. C'est conceptuellement  équivalent à une table dans une base de données relationnelle, un *DataFrame* en R ou en Python-pandas. Cette classe peut être obtenue de sources ou types de données variés : *Hive*, *json*, *xml*, *parquet*, *cassandra*... Ces fonctionnalités ne sont pas toutes introduites mais elles constituent un atout évident pour justifier du développement de cet environnement. Nous allons voir dans cette partie comment créer une *DataFrame* à partir d'un RDD, d'un fichier `.csv` ou d'un *DataFrame* `pandas`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Depuis un RDD\n",
    "\n",
    "Lecture du fichier dans un RDD. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = DATA_PATH+\"kddcup.data_10_percent.gz\"\n",
    "# Creation d'un RDD de chaines de caractères.\n",
    "string_rdd = sc.textFile(data_file).cache()\n",
    "# Creation d'un RDD de Liste de string.\n",
    "list_rdd = string_rdd.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Construction des ` Row` (lignes, une à une)\n",
    "La première opération consiste à construire le schéma des données. \n",
    "\n",
    "SparkSQL convertit en *DataFrame* un RDD composés d'objets `Row`.\n",
    "Une `Row` est comparable à un dictionnaire. Cet objet est construit en passant une liste de (clef, valeur) comme [kwargs](http://deusyss.developpez.com/tutoriels/Python/args_kwargs/).  La clef définit le nom de colonne et le type (entier, flottant...) est déduit de la première ligne. Il est donc important qu'il n'y ait pas de données manquantes dans la première ligne du RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "row_rdd = list_rdd.map(lambda p: Row(\n",
    "    duration=int(p[0]), \n",
    "    protocol_type=p[1],\n",
    "    service=p[2],\n",
    "    flag=p[3],\n",
    "    src_bytes=int(p[4]),\n",
    "    dst_bytes=int(p[5]),\n",
    "    interactions = p[-1],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois le RDD créé par lignes, le schéma est inféré puis enregistré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd_1 = sqlContext.createDataFrame(row_rdd)\n",
    "df_rdd_1.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spécifier le schéma\n",
    "\n",
    "Il est également possible de spécifier le schéma présent dans un RDD avant de le convertir en *DataFrame*\n",
    "Le schéma est créé grâce à l'objet `StructType` composé de `StructField` qui décrit les champs de valeur rencontrés dans le RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "fields = [StructField(\"duration\", IntegerType(), True),\n",
    "         StructField(\"protocol_type\", StringType(), True),\n",
    "          StructField(\"service\", StringType(), True),\n",
    "          StructField(\"flag\", StringType(), True),\n",
    "          StructField(\"src_bytes\", IntegerType(), True),\n",
    "          StructField(\"dst_bytes\", IntegerType(), True),\n",
    "          StructField(\"interactions\", StringType(), True)]\n",
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subList_rdd = list_rdd.map(lambda p: (int(p[0]), p[1], p[2], p[3], int(p[4]), int(p[5]), p[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd = sqlContext.createDataFrame(subList_rdd, schema)\n",
    "df_rdd.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Depuis un fichier `.csv`\n",
    "\n",
    "Les méthodes précédentes impliquent de connaitre la structure présente dans les RDD afin de spécifier le *schéma*.\n",
    "\n",
    "Lorsque le fichier source est dans un certain format connu (*parquet*, *json*, *csv*), on peut utiliser la fonction *spark.read.load* afin d'inférer automatiquement ce format dans un *dataframe*.\n",
    "\n",
    "Le fichier *kddcup.data_10_percent.gz* est organisé comme un fichier `.csv`. Le *dataframe* peut donc directement être lu à partir du fichier en intégrant la structure par colonne avec leur nom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.read.load(data_file, format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"False\")\n",
    "#Specify columns names\n",
    "df_csv=df_csv.toDF(*col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Depuis un *DataFrame* de `pandas`\n",
    "\n",
    "\n",
    "La conversion d'un *DataFrame* `pandas` vers un *DataFrame* `PysSpark` peut être effectuée grâce à la librairie *pyarrow* qui permet des transferts d'objets entre la *JVM* et *python*.\n",
    "\n",
    "Pour l'utiliser il est necessaire de changer la configuration `spark.sql.execution.arrow.enabled` à `true`. Celle-ci est à `False`par défaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lecture du fichier dans un *DataFrame pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pandas_df = pd.read_csv(DATA_PATH+\"kddcup.data_10_percent.gz\", sep=\",\", names=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Requête SQL\n",
    "### 3.1 Préalable\n",
    "\n",
    "*SparkSQL* permet d'appliquer une requête SQL sur un objet et de retourner le resultats ce cette requête sous le format *DataFrame*\n",
    "\n",
    "Pour cela, on enregistre dans un premier temps le *DataFrame* sous le format *SQL temporary view*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.createOrReplaceTempView(\"interactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Exemple de requête\n",
    "Des requêtes SQL peuvent ensuite être exécutées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélectionner les interactions \"tcp\" de plus de 1 s et sans transfert.\n",
    "tcp_interactions = sqlContext.sql(\"\"\"\n",
    "    SELECT duration, dst_bytes FROM interactions WHERE protocol_type = 'tcp' AND duration > 1000 AND dst_bytes \n",
    "    = 0\"\"\")\n",
    "tcp_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats d'une requête SQL sont des *DataFrames*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tcp_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sortie des durées avec les dst_bytes\n",
    "tcp_interactions_out = tcp_interactions.rdd.map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))\n",
    "for ti_out in tcp_interactions_out.collect():\n",
    "  print (ti_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impression du schéma du *DataFrame*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcp_interactions.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3.3  Autres requêtes SQL\n",
    "\n",
    "SparkSQL inclut un langage pour la manipulation de données structurées. Il permet de combiner des méthodes de sélection, filtrage, regroupement... des données. \n",
    " \n",
    "## 4 Opérations sur un *DataFrame* \n",
    "### 4.1 Opérations élémentaires\n",
    "\n",
    "`printSchema` permet d'afficher le type de chaque colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*select* permet d'extraire les colonnes d'une dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.select(\"interactions\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.select(\"interactions\",\"duration\").take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`groupBy` est équivalent à la fonction `groupby` de pandas.\n",
    "\n",
    "L'exemple ci-dessous compte le nombre d'interactions par type de protocole à l'aide de cette fonction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "df_rdd.groupBy(\"protocol_type\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Requete executee en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`filter` permet de selectionner les lignes d'un *DataFrame* selon une condition fixée sur une colonne.\n",
    "\n",
    "Pour compter les interactions de moins d'une seconde sans transfert de données et groupées par protocole, il suffit d'ajouter des filtres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "df_rdd.filter(df_rdd.duration>1000).filter(df_rdd.dst_bytes==0).groupBy(\"protocol_type\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Requete executee en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 `map` et *fonction utilisateur* pour *DataFrame*\n",
    "\n",
    "La fonction `map` n'est pas disponible sur les objets *DataFrame*. Une première façon de procéder consiste à convertir le *DataFrame* en objet RDD, pour appliquer la fonction `map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rdd.rdd.map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes)).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une autre façon consiste à convertir la fonction `lambda` en une *user defined function*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "function = udf(lambda x,y: \"Duration: {}, Dest. bytes: {}\".format(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce format permet ensuite d'appliquer cette fonction à la colonne ciblée du *DataFrame*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dataframe = df_rdd.select(function(\"duration\",\"dst_bytes\").alias(\"string_output\"))\n",
    "output_dataframe.take(3)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Creation d'une colonne\n",
    "\n",
    "La section précedente montre comment appliquer une fonction `udf` sur un *DataFrame*. Le résultat de cette fonction est un *DataFrame* d'une seule colonne. Il est possible d'ajouter cette colonne au *DataFrame* existant grâce à la fonction `withColumn`.\n",
    "\n",
    "\n",
    "Cette possibiité est illustrée, en ajoutant une colonne `label` au *DataFrame* `df`. Label ou variable qualitative avec deux modalités: `attack` et `normal`. \n",
    "\n",
    "Voici, dans un premier temps la fonction permettant de catégoriser chaque typr d'interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_or_normal_func(s):\n",
    "    return \"normal\" if s == \"normal.\" else \"attack\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction est convertie en *user defined function*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_or_normal = udf(attack_or_normal_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création de la nouvelle colonne `label` a partir du resultat de cette fonction à l'aide de la fonction `withColumn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_label = df.withColumn(\"label\", attack_or_normal(df.interactions) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vérification que la colonne \"label\" a bien été créée. Si le nom de colonne est déjà présent, celle-ci est remplacée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_label.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dénombrement du nombre d'attaques et d'interactions normales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "df_with_label.select(\"label\").groupBy(\"label\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Requete executee en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dénombrement par label et type de protocole pour souligner le pouvoir discriminant de cette variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "df_with_label.select(\"label\", \"protocol_type\").groupBy(\"label\", \"protocol_type\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Requete executee en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q** Que dire du protocole `udp`?\n",
    "\n",
    "Ajouter la prise en compte du transfert de données à partir de la cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "df_with_label.select(\"label\", \"protocol_type\", \"dst_bytes\").groupBy(\"label\", \"protocol_type\", df_with_label.dst_bytes==0).count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print (\"Requete executee en {} secondes\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Consulter les très nombreuses autres fonctionnalités, présentes ou à venir (version 2.3) dans la [documentation en ligne](http://spark.apache.org/docs/latest/sql-programming-guide.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Fonction Pandas `udf`\n",
    "\n",
    "Une fonction `pandas udf` est simialire à une `udf` définie précédemment. Mais elle permet de prendre en entrée une colonne d'un *DataFrame* qui sera traitée comme une *Séries* de *pandas*.\n",
    "\n",
    "Cette nouvelle classe de fonction permet une amélioration significative des [performances](https://databricks.com/blog/2017/10/30/introducing-vectorized-udfs-for-pyspark.html).\n",
    "\n",
    "Il existe deux types de `pandas udf`: `Scalar` et `Grouped Map`.\n",
    "\n",
    "#### `Scalar`\n",
    "\n",
    "Les *Scalar Pandas UDFs* sont utilisées pour vectoriser efficactement des opérations scalaires. Cette fonction prend en compte une *Series* de *pandas* comme argument et retourne une autre *Series* de *pandas* de la même taille tout en bénificiant des fonctions natives de *pandas*. \n",
    "\n",
    "Illustration avec la fonction `cumsum` de *pandas*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# Declare the function and create the UDF\n",
    "def cum_sum(x):\n",
    "    return x.cumsum()\n",
    "\n",
    "cum_sum_udf = pandas_udf(cum_sum, returnType=IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_sum_duration = df.select(cum_sum_udf(col(\"duration\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_sum_duration.take(1000)[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction *Grouped Map*\n",
    "\n",
    "Les fonctions *Grouped map Pandas UDFs* sont à utiliser avec la fonction `groupBy().apply()` permettant d'appliquer le pattern `split-apply-combine`. Ceci séalise en trois étapes:\n",
    "\n",
    " * *Split* des données en groupes à l'aide de `DataFrame.groupBy`.\n",
    " * *Apply* de la fonction sur chaque groupe. Les entrées et les sorties de la fonction sont des *DataFrames* de *pandas*. Les entrées de cette étape contiennent toutes les lignes et colonnes de chaque groupe. \n",
    " * *Combine* les résultats dans un nouveau *DataFrame*.\n",
    "\n",
    "Afin d'utiliser `groupBy().apply()`, il faut préalablement définir:\n",
    "\n",
    "* Une fonction python qui précise le calcul à exécuter sur chaque groupe.\n",
    "* Un objet `StructType`  ou une `string` qui définit le schéma du *DataFrame* de sortie.\n",
    "\n",
    "L'exemple suivant montre comment centrer la variable `duration` de chacun des groupes: `attack` et `normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(\"label string, duration int\", PandasUDFType.GROUPED_MAP)\n",
    "def substract_mean(pdf):\n",
    "    # pdf is a pandas.DataFrame\n",
    "    duration = pdf.duration\n",
    "    return pdf.assign(duration=duration - duration.mean())\n",
    "\n",
    "df_with_label.select(\"label\",\"duration\").groupby(\"label\").apply(substract_mean).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
